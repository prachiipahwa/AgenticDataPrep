import streamlit as st
import requests
import pandas as pd
import time

# ================= CONFIG =================
FASTAPI_URL = "http://127.0.0.1:8000"

st.set_page_config(
    page_title="Agentic Data Prep",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ================= CUSTOM CSS =================
st.markdown("""
<style>
.stApp {
    background: linear-gradient(135deg, #020617, #0f172a);
    color: white;
}
[data-testid="stSidebar"] {
    background: #020617;
}
h1, h2, h3 {
    color: #22d3ee;
}
.stButton>button {
    background: linear-gradient(90deg, #22d3ee, #3b82f6);
    color: black;
    font-weight: bold;
    border-radius: 10px;
    padding: 0.6rem 1.3rem;
    transition: 0.3s;
}
.stButton>button:hover {
    transform: scale(1.05);
}
.card {
    background: #020617;
    padding: 16px;
    border-radius: 14px;
    border: 1px solid #1e293b;
    margin-bottom: 12px;
}
.badge-ai {
    background: #16a34a;
    padding: 6px 14px;
    border-radius: 20px;
    font-weight: bold;
}
.badge-fallback {
    background: #facc15;
    padding: 6px 14px;
    border-radius: 20px;
    font-weight: bold;
    color: black;
}
</style>
""", unsafe_allow_html=True)

# ================= HERO =================
st.markdown("""
<h1 style="text-align:center;">ğŸ¤– Agentic Data Prep</h1>
<p style="text-align:center; font-size:17px; opacity:0.85;">
An Automated AI-Powered Data Cleaning and Preprocessing System
</p>
""", unsafe_allow_html=True)

# ================= SIDEBAR =================
st.sidebar.header("ğŸ“Œ Choose Input Method")

data_source = st.sidebar.radio(
    "Select data source",
    ["ğŸ“ Upload CSV / Excel", "ğŸ—„ï¸ Database Query", "ğŸŒ External API"]
)

# ---------- Sidebar Explanation ----------
st.sidebar.markdown("---")
st.sidebar.subheader("â„¹ï¸ What does this do?")

if data_source == "ğŸ“ Upload CSV / Excel":
    st.sidebar.markdown("""
    **Upload CSV / Excel**

    Upload datasets directly from your system.

    **Agent Workflow:**
    - Detects missing values & duplicates  
    - Fixes data types and normalizes values  
    - Enhances cleaning using AI when available  
    - Falls back to traditional methods if AI fails  

    ğŸ“¥ Output: Cleaned, model-ready dataset
    """)

elif data_source == "ğŸ—„ï¸ Database Query":
    st.sidebar.markdown("""
    **Database Query**

    Fetch raw data directly from PostgreSQL or MySQL databases.

    **Agent Workflow:**
    - Executes the provided SQL query  
    - Cleans the fetched data automatically  
    - Applies AI enhancement when available  
    - Guarantees output via fallback system  

    ğŸ“¥ Output: Cleaned query results
    """)

elif data_source == "ğŸŒ External API":
    st.sidebar.markdown("""
    **External API**

    Fetch raw JSON data from REST APIs.

    **Agent Workflow:**
    - Converts JSON to structured tabular data  
    - Handles inconsistencies and missing fields  
    - Uses AI for intelligent formatting  
    - Ensures valid output even without AI  

    ğŸ“¥ Output: Cleaned structured dataset
    """)

# ================= HELPER =================
def show_agent_status(ai_enhanced, message):
    if ai_enhanced:
        st.markdown(f"""
        <div class="card">
            <span class="badge-ai">ğŸ¤– AI AGENT ENHANCED</span>
            <p>{message}</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="card">
            <span class="badge-fallback">âš ï¸ FALLBACK MODE</span>
            <p>{message}</p>
        </div>
        """, unsafe_allow_html=True)

# ================= CSV / EXCEL =================
if data_source == "ğŸ“ Upload CSV / Excel":
    st.subheader("ğŸ“ Upload Dataset")

    uploaded_file = st.file_uploader(
        "Supported formats: CSV, Excel",
        type=["csv", "xlsx", "xls"]
    )

    if uploaded_file:
        df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)

        st.subheader("ğŸ” Raw Data Preview")
        st.dataframe(df.head(10), use_container_width=True)

        col1, col2, col3 = st.columns(3)
        col1.metric("Rows", df.shape[0])
        col2.metric("Columns", df.shape[1])
        col3.metric("Missing Values", df.isnull().sum().sum())

        if st.button("ğŸš€ Run Agentic Data Cleaning"):
            progress = st.progress(0)
            for i in range(70):
                time.sleep(0.01)
                progress.progress(i + 1)

            response = requests.post(
                f"{FASTAPI_URL}/cleandata/",
                files={"file": (uploaded_file.name, uploaded_file.getvalue())}
            )

            if response.status_code == 200:
                result = response.json()
                cleaned_df = pd.DataFrame(result.get("cleaned_data", []))

                show_agent_status(
                    result.get("ai_enhanced", False),
                    result.get("message", "")
                )

                tab1, tab2 = st.tabs(["ğŸ“Š Cleaned Data", "ğŸ“ˆ Statistical Summary"])
                with tab1:
                    st.dataframe(cleaned_df, use_container_width=True)
                with tab2:
                    st.write(cleaned_df.describe())

                st.download_button(
                    "ğŸ“¥ Download Cleaned Dataset",
                    cleaned_df.to_csv(index=False),
                    "cleaned_data.csv",
                    "text/csv"
                )
            else:
                st.error("âŒ Backend not reachable. Ensure FastAPI is running.")

# ================= DATABASE =================
elif data_source == "ğŸ—„ï¸ Database Query":
    st.subheader("ğŸ—„ï¸ Database Ingestion")

    db_url = st.text_input(
        "Database Connection URL",
        "postgresql://user:password@localhost:5432/db"
    )
    query = st.text_area("SQL Query", "SELECT * FROM my_table;")

    if st.button("ğŸš€ Fetch & Run Agent"):
        with st.spinner("Invoking Agentic Data Prep pipeline..."):
            response = requests.post(
                f"{FASTAPI_URL}/clean-db/",
                json={"db_url": db_url, "query": query}
            )

        if response.status_code == 200:
            result = response.json()
            cleaned_df = pd.DataFrame(result.get("cleaned_data", []))

            show_agent_status(result.get("ai_enhanced", False), result.get("message", ""))
            st.dataframe(cleaned_df, use_container_width=True)

            st.download_button(
                "ğŸ“¥ Download Cleaned Dataset",
                cleaned_df.to_csv(index=False),
                "cleaned_data.csv",
                "text/csv"
            )
        else:
            st.error("âŒ Database cleaning failed")

# ================= API =================
elif data_source == "ğŸŒ External API":
    st.subheader("ğŸŒ API Data Fetching")

    api_url = st.text_input(
        "API Endpoint",
        "https://jsonplaceholder.typicode.com/posts"
    )

    if st.button("ğŸš€ Fetch & Run Agent"):
        with st.spinner("Agent analyzing API data..."):
            response = requests.post(
                f"{FASTAPI_URL}/clean-api/",
                json={"api_url": api_url}
            )

        if response.status_code == 200:
            result = response.json()
            cleaned_df = pd.DataFrame(result.get("cleaned_data", []))

            show_agent_status(result.get("ai_enhanced", False), result.get("message", ""))
            st.dataframe(cleaned_df, use_container_width=True)

            st.download_button(
                "ğŸ“¥ Download Cleaned Dataset",
                cleaned_df.to_csv(index=False),
                "cleaned_data.csv",
                "text/csv"
            )
        else:
            st.error("âŒ API cleaning failed")

# ================= FOOTER =================
st.markdown("""
<hr>
<p style="text-align:center; opacity:0.7;">
<b>Agentic Data Prep</b> â€” An Automated AI-Powered Data Cleaning and Preprocessing System<br>
Built using Streamlit Â· FastAPI Â· Groq AI Â· LangGraph
</p>
""", unsafe_allow_html=True)

