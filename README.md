# ğŸ¤– Agentic Data Prep
### An Autonomous AI-Powered Data Cleaning and Preprocessing System 

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.10">
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI">
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit">
  <img src="https://img.shields.io/badge/Groq-FF6B00?style=for-the-badge&logo=ai&logoColor=white" alt="Groq AI">
  <img src="https://img.shields.io/badge/LangGraph-121212?style=for-the-badge" alt="LangGraph">
</p>

<p align="center">
  <strong>An intelligent AI-powered system with graceful fallback that automates data cleaning and preprocessing tasks for machine learning workflows.</strong>
</p>

---

## ğŸ“– Overview

This project leverages **Python, FastAPI, Streamlit, and Groq AI (LLaMA 3.3)** with **LangGraph** to streamline data preparation, ensuring clean, consistent, and model-ready datasets. It combines traditional data cleaning techniques with AI-powered intelligent processing, featuring a **resilient fallback mechanism** that guarantees results even when AI services are unavailable.

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¤– **AI-Powered Data Cleaning** | Automatically detects and handles missing values, outliers, and inconsistencies using Groq AI (LLaMA 3.3) |
| ğŸ›¡ï¸ **Graceful Fallback System** | Always returns cleaned data - uses traditional methods if AI quota is exceeded |
| ğŸ§© **Automated Preprocessing** | Performs encoding, normalization, and scaling operations efficiently |
| ğŸ—„ï¸ **Database Integration** | Ingests raw data directly from **PostgreSQL** and **MySQL** databases |
| ğŸŒ **API Data Fetching** | Fetch and clean data from external REST APIs |
| ğŸ“ **File Upload Support** | Upload and process CSV and Excel files |
| âš™ï¸ **RESTful API Backend** | Real-time endpoints via **FastAPI** for seamless integration with ML pipelines |
| ğŸ§  **AI Agent with LangGraph** | Uses **LangGraph StateGraph** for intelligent workflow orchestration |
| ğŸ–¥ï¸ **Modern Web UI** | Beautiful **Streamlit** interface with status indicators and CSV download |
| ğŸ“Š **Smart Status Labels** | Clear indicators showing whether AI or traditional cleaning was used |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STREAMLIT UI                              â”‚
â”‚   (Upload CSV/Excel â”‚ Database Query â”‚ External API)            â”‚
â”‚   â€¢ Status Labels (AI âœ… / Traditional âš ï¸)                      â”‚
â”‚   â€¢ Download Button ğŸ“¥                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FASTAPI BACKEND                              â”‚
â”‚     Endpoints: /cleandata/ â”‚ /clean-db/ â”‚ /clean-api/           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DATA CLEANING        â”‚    â”‚         AI AGENT                 â”‚
â”‚  (Traditional Methods)  â”‚ â†’  â”‚  (Groq AI + LangGraph)           â”‚
â”‚  âœ… ALWAYS WORKS        â”‚    â”‚  â“ MAY FAIL (quota/rate limit) â”‚
â”‚  â€¢ Handle missing vals  â”‚    â”‚  â€¢ Intelligent imputation        â”‚
â”‚  â€¢ Remove duplicates    â”‚    â”‚  â€¢ Anomaly detection             â”‚
â”‚  â€¢ Min-Max normalize    â”‚    â”‚  â€¢ Context-aware formatting      â”‚
â”‚  â€¢ Fix data types       â”‚    â”‚  â€¢ Returns cleaned JSON          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                               â”‚
              â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚         â”‚ AI Success?
              â”‚         â”œâ”€â”€â”€ YES â†’ Use AI result
              â”‚         â””â”€â”€â”€ NO  â†’ Use Traditional result
              â”‚                               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      CLEANED DATA (JSON)            â”‚
              â”‚   â€¢ cleaned_data: [...]             â”‚
              â”‚   â€¢ ai_enhanced: true/false         â”‚
              â”‚   â€¢ message: status description     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
Ai_Agent_for_Data_Cleaning_and_Preprocessing/
â”‚
â”œâ”€â”€ ğŸ“„ .env                      # Environment variables (API keys, DB config)
â”œâ”€â”€ ğŸ“„ README.md                 # Project documentation
â”œâ”€â”€ ğŸ“„ requirement.txt           # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â””â”€â”€ ğŸ“„ app.py                # Streamlit frontend application
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ ğŸ“„ sample_data.csv       # Sample CSV dataset
â”‚   â””â”€â”€ ğŸ“„ sample_data.xlsx      # Sample Excel dataset
â”‚
â””â”€â”€ ğŸ“‚ Scripts/
    â”œâ”€â”€ ğŸ“„ ai_agent.py           # AI Agent with Gemini & LangGraph
    â”œâ”€â”€ ğŸ“„ backend.py            # FastAPI backend server
    â”œâ”€â”€ ğŸ“„ data_cleaning.py      # Traditional data cleaning methods
    â”œâ”€â”€ ğŸ“„ data_ingestion.py     # Data loading utilities
    â”œâ”€â”€ ğŸ“„ main.py               # Main entry point
    â”œâ”€â”€ ğŸ“„ test_mysql_connection.py    # MySQL connection test
    â””â”€â”€ ğŸ“„ test_postgre_connection.py  # PostgreSQL connection test
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|----------|------------|
| **Frontend** | Streamlit |
| **Backend** | FastAPI, Uvicorn |
| **AI/ML** | Groq AI (LLaMA 3.3), LangGraph |
| **Data Processing** | Pandas, NumPy, Scikit-learn |
| **Database** | PostgreSQL, MySQL (via SQLAlchemy) |
| **HTTP Client** | Requests, AIOHTTP |

---

## ğŸš€ Getting Started

### Prerequisites

#### Python Version Requirements

**âœ… Recommended: Python 3.10**

This project is fully tested and optimized for **Python 3.10**. Here's the compatibility breakdown:

| Python Version | Status | Notes |
|----------------|--------|-------|
| **3.10.x** | âœ… **Fully Supported** | Recommended version - all packages compatible |
| **3.11.x** | âœ… Supported | Most features work, some packages may need updates |
| **3.12.x** | âš ï¸ Limited Support | Some dependencies may not be fully compatible |
| **3.13.x** | âŒ Not Supported | Several packages incompatible (scipy, ipython) |
| **3.9.x** | âš ï¸ Limited Support | Some newer features may not work |

**Why Python 3.10?**
- All AI/ML libraries (LangGraph, Google Gemini, scipy, scikit-learn) are fully compatible
- Best balance between modern features and package compatibility
- Stable and widely used in production environments

#### Other Requirements

- **Groq API Key** (Required for AI features - free tier available)
- **PostgreSQL or MySQL** (Optional, only needed for database features)
- **Git** (for cloning the repository)

---

### Installation

#### Step 1: Clone the Repository
```bash
git clone https://github.com/prachiipahwa/AgenticDataPrep.git
cd "AgenticDataPrep"
```

#### Step 2: Create Virtual Environment with Python 3.10

**On macOS/Linux:**
```bash
# Using python3.10 specifically
python3.10 -m venv .venv
source .venv/bin/activate
```

**On Windows:**
```cmd
# Using python3.10 specifically
python3.10 -m venv .venv
.venv\Scripts\activate
```

**Alternative: Using conda**
```bash
conda create -n dataprep python=3.10
conda activate dataprep
```

#### Step 3: Install Dependencies
```bash
pip install --upgrade pip
pip install -r requirement.txt
```

**Note:** If you encounter any issues during installation, try:
```bash
# Install packages one category at a time
pip install fastapi uvicorn starlette python-multipart
pip install pandas numpy openpyxl
pip install google-generativeai langgraph
pip install streamlit
pip install python-dotenv
```

#### Step 4: Configure Environment Variables

Create a `.env` file in the root directory:

```env
# Required: Groq API Key
GROQ_API_KEY=your_groq_api_key_here

# Optional: MySQL Configuration (only if using database features)
MYSQL_HOST=127.0.0.1
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=your_password
MYSQL_DATABASE=your_database

# Optional: PostgreSQL Configuration
POSTGRES_HOST=127.0.0.1
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_DATABASE=your_database
```

**Get your Groq API Key:**
1. Visit [Groq Console](https://console.groq.com/)
2. Sign up or log in with your account
3. Navigate to API Keys section
4. Click "Create API Key"
5. Copy the key and paste it in your `.env` file

**Why Groq?**
- âš¡ **Ultra-fast inference** - 10x faster than traditional cloud AI
- ğŸ†“ **Generous free tier** - No credit card required
- ğŸ§  **LLaMA 3.3 70B** - State-of-the-art open-source model
- ğŸ›¡ï¸ **Reliable** - Built-in rate limiting and error handling

---

### Running the Application

#### Option 1: Using Two Terminals (Recommended)

**Terminal 1 - Start FastAPI Backend:**
```bash
# Make sure you're in the project root directory
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uvicorn Scripts.backend:app --host 127.0.0.1 --port 8000 --reload
```

**Terminal 2 - Start Streamlit Frontend:**
```bash
# In a new terminal window
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
streamlit run app/app.py
```

#### Option 2: Using Background Process

```bash
# Start backend in background
source .venv/bin/activate
uvicorn Scripts.backend:app --host 127.0.0.1 --port 8000 --reload &

# Start frontend
streamlit run app/app.py
```

#### Option 3: Quick Start Script

Create a `start.sh` file (macOS/Linux):
```bash
#!/bin/bash
source .venv/bin/activate
uvicorn Scripts.backend:app --host 127.0.0.1 --port 8000 --reload &
streamlit run app/app.py
```

Make it executable and run:
```bash
chmod +x start.sh
./start.sh
```

#### Access the Application

Once both servers are running:
- **Streamlit Frontend:** [http://localhost:8501](http://localhost:8501)
- **FastAPI Backend:** [http://127.0.0.1:8000](http://127.0.0.1:8000)
- **API Documentation:** [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
- **Alternative API Docs:** [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)

---

### Troubleshooting Installation

<details>
<summary><strong>âŒ "ModuleNotFoundError: No module named 'dotenv'"</strong></summary>

**Solution:**
```bash
pip install python-dotenv
```
</details>

<details>
<summary><strong>âŒ "No module named 'langgraph'"</strong></summary>

**Solution:**
```bash
pip install langgraph google-generativeai
```
</details>

<details>
<summary><strong>âŒ "Could not find a version that satisfies the requirement scipy==1.16.2"</strong></summary>

**Solution:** This means you're using Python 3.13. Use Python 3.10 instead:
```bash
# Recreate virtual environment with Python 3.10
rm -rf .venv
python3.10 -m venv .venv
source .venv/bin/activate
pip install -r requirement.txt
```
</details>

<details>
<summary><strong>âŒ "Connection refused" when uploading files in Streamlit</strong></summary>

**Solution:** Make sure the FastAPI backend is running:
```bash
# In a separate terminal
uvicorn Scripts.backend:app --host 127.0.0.1 --port 8000 --reload
```
</details>

<details>
<summary><strong>âŒ "GEMINI_API_KEY is missing"</strong></summary>

**Solution:** Create or update your `.env` file with a valid API key:
```env
GEMINI_API_KEY=your_actual_api_key_here
```
</details>

---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/cleandata/` | Upload and clean CSV/Excel files |
| `POST` | `/clean-db/` | Fetch and clean data from database |
| `POST` | `/clean-api/` | Fetch and clean data from external API |

### Example: Clean a CSV file

```bash
curl -X POST "http://localhost:8000/cleandata/" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@data/sample_data.csv"
```

### Example: Clean data from database

```bash
curl -X POST "http://localhost:8000/clean-db/" \
  -H "Content-Type: application/json" \
  -d '{
    "db_url": "postgresql://user:password@localhost:5432/mydb",
    "query": "SELECT * FROM my_table"
  }'
```

---

## ğŸ§¹ Data Cleaning Operations

### Traditional Cleaning (`data_cleaning.py`) - âœ… Always Runs
- âœ… Handle missing values (mean, median, mode, drop)
- âœ… Remove duplicate rows
- âœ… Fix and convert data types
- âœ… **Min-Max normalization** (scales values 0-1)

### AI-Powered Cleaning (`ai_agent.py`) - â“ Optional Enhancement
- ğŸ¤– Intelligent missing value imputation
- ğŸ¤– Context-aware data formatting
- ğŸ¤– Anomaly detection and handling
- ğŸ¤– Text standardization

### ğŸ›¡ï¸ Graceful Fallback System

The system **always returns cleaned data**, even if AI fails:

| Scenario | Result |
|----------|--------|
| AI Success | Traditional cleaning + AI enhancement |
| AI Quota Exceeded | Traditional cleaning only |
| AI Rate Limited | Traditional cleaning only |
| AI Service Down | Traditional cleaning only |

**Response Format:**
```json
{
  "cleaned_data": [...],
  "ai_enhanced": true/false,
  "message": "Data cleaned successfully with AI enhancement"
}
```

---

## ğŸ–¼ï¸ Screenshots

### Data Source Selection
The application supports three data input methods:
- ğŸ“ Upload CSV/Excel files
- ğŸ—„ï¸ Query databases directly
- ğŸŒ Fetch from external APIs

### Cleaned Data Output
View the cleaned data in a beautiful, interactive table format with the ability to download the results.

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `GROQ_API_KEY` | Groq AI API key for LLaMA 3.3 | âœ… Yes |
| `MYSQL_HOST` | MySQL database host | âŒ Optional |
| `MYSQL_PORT` | MySQL database port | âŒ Optional |
| `MYSQL_USER` | MySQL username | âŒ Optional |
| `MYSQL_PASSWORD` | MySQL password | âŒ Optional |
| `MYSQL_DATABASE` | MySQL database name | âŒ Optional |

---

## ğŸ§ª Testing

### Test Database Connections

```bash
# Test MySQL connection
python Scripts/test_mysql_connection.py

# Test PostgreSQL connection
python Scripts/test_postgre_connection.py
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Groq](https://groq.com/) for blazing-fast AI inference
- [Meta AI](https://ai.meta.com/) for the LLaMA 3.3 model
- [LangGraph](https://github.com/langchain-ai/langgraph) for AI workflow orchestration
- [FastAPI](https://fastapi.tiangolo.com/) for the amazing web framework
- [Streamlit](https://streamlit.io/) for the beautiful UI components

---
